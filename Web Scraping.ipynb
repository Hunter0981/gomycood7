{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77f64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    Get and parse HTML content from a Wikipedia page\n",
    "\n",
    "    :param url: str, the URL of the Wikipedia page to parse\n",
    "    :return: bs4.BeautifulSoup object, the parsed HTML content of the Wikipedia page\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_title(soup):\n",
    "    \"\"\"\n",
    "    Extract the title of the Wikipedia article\n",
    "\n",
    "    :param soup: bs4.BeautifulSoup object, the parsed HTML content of the Wikipedia page\n",
    "    :return: str, the title of the Wikipedia article\n",
    "    \"\"\"\n",
    "    title = soup.find('h1', id='firstHeading').text\n",
    "    return title\n",
    "\n",
    "\n",
    "def extract_text(soup):\n",
    "    \"\"\"\n",
    "    Extract the text of the Wikipedia article, along with the headings for each section\n",
    "\n",
    "    :param soup: bs4.BeautifulSoup object, the parsed HTML content of the Wikipedia page\n",
    "    :return: dict, a dictionary containing the headings as keys and the text for each section as values\n",
    "    \"\"\"\n",
    "    text = {}\n",
    "    content = soup.find('div', id='content')\n",
    "    body = content.find('div', id='bodyContent')\n",
    "    for section in body.find_all(['h2', 'h3']):\n",
    "        section_title = section.text\n",
    "        section_text = ''\n",
    "        for element in section.next_siblings:\n",
    "            if element.name and element.name.startswith('h'):\n",
    "                break\n",
    "            if element.name == 'p':\n",
    "                section_text += element.text\n",
    "        text[section_title] = section_text\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_links(soup):\n",
    "    \"\"\"\n",
    "    Collect all links in the Wikipedia page that redirects to another Wikipedia page\n",
    "\n",
    "    :param soup: bs4.BeautifulSoup object, the parsed HTML content of the Wikipedia page\n",
    "    :return: list, a list of all links that redirect to another Wikipedia page\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('href') and link['href'].startswith('/wiki/'):\n",
    "            links.append('https://en.wikipedia.org' + link['href'])\n",
    "    return links\n",
    "\n",
    "\n",
    "def scrape_wikipedia(url):\n",
    "    \"\"\"\n",
    "    Scrape text from a Wikipedia page\n",
    "\n",
    "    :param url: str, the URL of the Wikipedia page to scrape\n",
    "    :return: dict, a dictionary containing the title, text, and links of the Wikipedia page\n",
    "    \"\"\"\n",
    "    soup = get_html(url)\n",
    "    title = extract_title(soup)\n",
    "    text = extract_text(soup)\n",
    "    links = extract_links(soup)\n",
    "    result = {'title': title, 'text': text, 'links': links}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fada6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
